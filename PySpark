####################
BIG DATA

Big data refers to extremely large and complex datasets that traditional data processing software can't effectively manage. It's characterized by the 
"Three Vs": Volume, Velocity, and Variety, with some definitions adding Variability and Veracity. Here's a detailed breakdown of these aspects:

### 1. Volume
- **Definition**: Refers to the sheer amount of data generated and collected. This can range from terabytes to petabytes and beyond.
- **Sources**: Data can come from various sources, including social media, sensors, transactional data, and more.
- **Challenges**: Storing and processing such large volumes requires scalable storage solutions and robust computational power.

### 2. Velocity
- **Definition**: This aspect addresses the speed at which data is generated and processed. Data can be created in real-time or near-real-time.
- **Examples**: Streaming data from social media, financial transactions, or IoT devices.
- **Challenges**: The need for real-time processing and analysis can overwhelm traditional data management systems.

### 3. Variety
- **Definition**: Refers to the different types of data (structured, semi-structured, unstructured) that can be collected.
- **Types**:
  - **Structured Data**: Organized data, typically in databases (e.g., SQL databases).
  - **Unstructured Data**: Data that doesn't have a predefined structure (e.g., text, images, videos).
  - **Semi-Structured Data**: Hybrid data formats that may contain both structured and unstructured elements (e.g., JSON, XML).
- **Challenges**: Integrating and analyzing diverse data types can complicate data management.

### 4. Variability
- **Definition**: Refers to the inconsistency of the data, which can change rapidly. This can impact data quality and analysis.
- **Example**: Seasonal trends in consumer behavior can create fluctuations in data patterns.
- **Challenges**: Maintaining data accuracy and relevance in the face of variability.

### 5. Veracity
- **Definition**: Addresses the quality and reliability of the data.
- **Issues**: Data can be noisy, incomplete, or biased, which can affect the outcomes of data analysis.
- **Challenges**: Ensuring data quality through validation and cleansing processes.




######################
HADOOP vs PYSPARK

1. Core Technology
Hadoop:
Hadoop is an open-source framework used for distributed storage and processing of large datasets. It primarily consists of 
two components:
HDFS (Hadoop Distributed File System): A distributed file system to store large datasets.
MapReduce: A programming model and engine for processing and generating large datasets in parallel.
PySpark:
PySpark is the Python API for Apache Spark, which is a fast, in-memory data processing engine. Spark is designed to be more 
efficient than Hadoop’s MapReduce by using memory for faster computation, which is often better suited for real-time analytics.

2. Programming Model
Hadoop:
MapReduce is the primary programming model for processing data in Hadoop. It involves splitting tasks into smaller sub-
tasks (mappers) and then combining the results (reducers). It's batch processing-based, which can be slower.
PySpark:
Spark uses a more flexible programming model, supporting APIs like RDDs (Resilient Distributed Datasets) and DataFrames for 
distributed data processing. PySpark provides a Python interface to interact with Spark’s capabilities, offering higher-level 
functions and faster processing than MapReduce.

3. Performance
Hadoop:
Hadoop’s MapReduce is disk-based, which means it stores intermediate data on disk. This can result in slower performance 
due to high I/O operations.
PySpark:
Spark is in-memory, meaning it processes data in RAM, significantly speeding up computations. This makes Spark much faster 
for iterative machine learning and real-time analytics tasks compared to Hadoop.

4. Ease of Use
Hadoop:
Writing MapReduce jobs can be complex and requires knowledge of Java (though other languages like Python can be used 
with additional frameworks).
PySpark:
PySpark is more user-friendly, especially for Python developers, because it provides a simpler and higher-level API to
process data using Python libraries.

5. Real-Time Processing
Hadoop:
Hadoop is primarily designed for batch processing. It’s not suited for real-time data processing.
PySpark:
Spark supports both batch processing and real-time stream processing with Spark Streaming. This makes it more versatile
for modern data processing needs, including real-time analytics.

6. Fault Tolerance
Hadoop:
Hadoop provides fault tolerance through data replication in HDFS. If a node fails, the data is replicated on other nodes 
to ensure no data loss.
PySpark:
Spark provides fault tolerance through RDDs. If a task fails, Spark recomputes the lost data using lineage information, 
ensuring no data loss.

7. Scalability
Hadoop:
Hadoop scales well by distributing data across a cluster and processing it in parallel. It is designed to handle petabytes
of data.
PySpark:
Spark also scales efficiently and is optimized for speed. It can process data in parallel across a cluster and handle 
very large datasets.

8. Use Cases
Hadoop:
Hadoop is typically used for batch processing and large-scale data storage. It’s well-suited for ETL (Extract, Transform, Load)
tasks, data warehousing, and data archiving.
PySpark:
PySpark is used for data analysis, machine learning, real-time processing, and data processing pipelines. It is popular 
for tasks like big data analytics, ETL, and processing streaming data.

9. Ecosystem
Hadoop:
Hadoop has a rich ecosystem of related tools, including:
Hive: SQL-like interface for querying data.
Pig: Scripting language for data processing.
HBase: NoSQL database for real-time access to large datasets.
PySpark:
PySpark integrates with the broader Apache Spark ecosystem, which includes tools like:
MLlib: Machine learning library.
GraphX: Graph processing framework.
Spark SQL: SQL-based querying interface.


#######################
PySPARK ARCHITECTURE

1. Driver Program: The Driver Program is the main entry point for PySpark applications. It runs the user’s code and coordinates the execution of tasks across the cluster.
The Driver Program communicates with the Cluster Manager to allocate resources, schedule tasks, and monitor their progress. It also manages the SparkContext, which is the
entry point for interacting with Spark from Python.

2. Cluster Manager: The Cluster Manager is responsible for managing the resources and scheduling tasks on a cluster of machines. It can be one of the supported cluster
managers, such as Apache Mesos, Hadoop YARN, or Spark’s standalone cluster manager. The Cluster Manager allocates resources to the application based on the requested 
configuration and manages the distribution of tasks to the available workers.

3. Executors: Executors are worker processes that run on the cluster nodes and perform the actual computations. They are responsible for executing the tasks assigned to
them by the Driver Program. Executors manage the data partitions and cache intermediate results in memory for efficient processing. They communicate with the Driver 
Program and the Cluster Manager to receive tasks, report task status, and exchange data.

4. Resilient Distributed Dataset (RDD): RDD is the fundamental data structure in PySpark. It represents an immutable distributed collection of objects across the cluster. 
RDDs are partitioned and distributed across multiple nodes in the cluster, allowing for parallel processing of data. RDDs are fault-tolerant, meaning they can recover 
from node failures by using their lineage information to reconstruct lost partitions.

Spark -> Scala(Advanced Java) -> provides API so its compatible with Python and Java 
Architecture -> Master node(Driver Code > Spark Content)
                Worker Nodes connected with master nodes using cluster manager

Spark Context is like the DB
RDD - Resililient (means flexible) Distributed Dataset  
RDD is schemaless

DAG - Directed Acyclic Graph is created upon creating RDD


#####################
DAG AND LAZY EVALUATION
DAG (Directed Acyclic Graph). DAG is a crucial concept that underpins how Spark executes operations on data. 

A DAG is a graph where:
- The nodes represent computations or operations.
- The edges represent dependencies between these operations.
- "Acyclic" means that there are no cycles (i.e., no circular dependencies).

When you perform a transformation (e.g., `map()`, `filter()`, `join()`) on an RDD (Resilient Distributed Dataset) or a DataFrame in PySpark, Spark constructs a 
DAG to represent the sequence of operations.

#### **How is DAG used in PySpark?**
- **Stage creation**: The transformations are divided into stages based on data shuffling. Each stage contains a set of narrow transformations (e.g., `map`, `filter`, etc.) that can be computed without data shuffling. These operations can be executed in parallel. However, wide transformations (like `groupByKey`, `reduceByKey`, etc.) will cause shuffling, and thus a new stage is created.
- **Execution plan**: The DAG is used by Spark’s scheduler to create an optimized execution plan that minimizes the number of stages and data shuffling required to complete the job.
- **Fault tolerance**: If a partition of data is lost, the DAG can be recomputed from the original dataset, ensuring that no data is lost and computations can continue.

#### **Example of DAG in PySpark:**
Here’s a simple example to illustrate how DAG is used in PySpark.

```python
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.master("local").appName("DAG Example").getOrCreate()

# Sample data
data = [("Alice", 25), ("Bob", 30), ("Catherine", 35), ("David", 40)]
df = spark.createDataFrame(data, ["name", "age"])

# Transformation 1: Filter the people whose age is greater than 30
filtered_df = df.filter(df.age > 30)

# Transformation 2: Add a new column with a modified age
modified_df = filtered_df.withColumn("age_plus_five", df.age + 5)

# Action: Show the result
modified_df.show()
```

**DAG Execution Breakdown**:
1. The first transformation (`filter`) creates a stage where data is filtered.
2. The second transformation (`withColumn`) creates another stage that adds a new column.
3. The **action** (`show`) triggers the execution, where Spark builds a DAG based on these transformations, divides it into stages, and performs the required computations.

### **Lazy Evaluation in PySpark:**

**Lazy evaluation** is an optimization technique used by Spark where transformations are not executed immediately when called. Instead, Spark builds an execution plan (a DAG) that describes the transformations, but no actual computation is done until an action is called (e.g., `collect()`, `show()`, `count()`).

This allows Spark to:
1. **Optimize the execution plan**: Since the operations are only recorded and not executed immediately, Spark can optimize the DAG by merging stages and minimizing shuffling.
2. **Avoid unnecessary computations**: It can avoid computing intermediate results that may not be needed for the final result.

#### **How Lazy Evaluation Works in PySpark:**
- **Transformation**: Operations like `map()`, `filter()`, `select()`, `withColumn()`, etc., are transformations. These transformations define the structure of the resulting RDD/DataFrame but do not perform any computation.
- **Action**: Actions like `collect()`, `show()`, `count()`, `save()`, etc., trigger the actual computation. These actions will materialize the transformations and execute them.

#### **Example of Lazy Evaluation:**

```python
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.master("local").appName("Lazy Evaluation Example").getOrCreate()

# Sample data
data = [("Alice", 25), ("Bob", 30), ("Catherine", 35), ("David", 40)]
df = spark.createDataFrame(data, ["name", "age"])

# Lazy transformation: filter records where age is greater than 30
filtered_df = df.filter(df.age > 30)

# Lazy transformation: add 5 to age
modified_df = filtered_df.withColumn("age_plus_five", filtered_df.age + 5)

# Action: Show the result
modified_df.show()
```

**Lazy Evaluation Process**:
1. **`filter(df.age > 30)`**: This transformation will not immediately execute. Spark just records that a filter operation is applied.
2. **`withColumn("age_plus_five", ...)`**: Similarly, this transformation is recorded but not executed immediately.
3. **`show()`**: When the `show()` action is called, Spark will execute all the recorded transformations. Spark will:
   - Build a DAG.
   - Optimize it (e.g., combine the transformations if possible).
   - Execute the plan efficiently by distributing the work across the cluster.

### **Benefits of Lazy Evaluation in PySpark:**
1. **Optimization**: Spark can optimize the execution plan by analyzing the entire DAG before execution. For instance, it may eliminate redundant transformations, minimize the amount of data shuffled between nodes, or re-order operations to improve performance.
2. **Efficiency**: Since transformations are not executed until an action is called, Spark can minimize the computation and resource consumption by avoiding unnecessary intermediate steps.

### **Summary of Lazy Evaluation and DAG in PySpark:**
- **DAG**: Spark constructs a DAG to represent the sequence of operations performed on RDDs/DataFrames. This helps in parallel processing, fault tolerance, and efficient execution of tasks.
- **Lazy Evaluation**: Spark uses lazy evaluation for transformations. It builds the DAG without performing actual computations until an action is triggered. This allows Spark to optimize the execution plan and minimize unnecessary computations.

By combining DAG and lazy evaluation, PySpark can efficiently process large datasets across a distributed environment, optimizing performance and reducing unnecessary resource consumption.



#################
CREATION OF RDD

-> to parallelize(...) some collection of elements
-> by referencing a file located locally or externally


###################
TRANSFORMATIONS IN RDD

transformations are of 2 types - narrow and wide

-> map() : Applies a function func to each element in the RDD and returns a new RDD consisting of the results of the function.
rdd = sc.parallelize([1, 2, 3, 4])
squared_rdd = rdd.map(lambda x: x ** 2)

-> filter():Applies a function func to each element and returns an RDD that contains only the elements where func returns true.
rdd = sc.parallelize([1, 2, 3, 4, 5])
filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # Keep only even numbers

-> flatMap():Similar to map(), but each input item can be mapped to 0 or more output items (so func should return a sequence rather than a single item).
rdd = sc.parallelize(["hello world", "how are you"])
flat_mapped_rdd = rdd.flatMap(lambda x: x.split(" "))  # Split each sentence into words

-> disctinct():Returns a new RDD containing distinct elements from the original RDD.
rdd = sc.parallelize([1, 2, 2, 3, 3, 4])
distinct_rdd = rdd.distinct()

-> sample(withReplacement, fraction, seed=None):
rdd = sc.parallelize(range(100))
sampled_rdd = rdd.sample(False, 0.1)  # Sample 10% of the data without replacement

-> sortByKey(ascending=True):
rdd = sc.parallelize([(2, 'b'), (1, 'a'), (3, 'c')])
sorted_rdd = rdd.sortByKey()  # Sort by key in ascending order

-> union(other_rdd) and intersection(other_rdd):
rdd1 = sc.parallelize([1, 2, 3])
rdd2 = sc.parallelize([3, 4, 5])
union_rdd = rdd1.union(rdd2)
intersection_rdd = rdd1.intersection(rdd2)

-> groupByKey() and reduceByKey(func):
rdd = sc.parallelize([(1, 'a'), (2, 'b'), (1, 'c')])
grouped_rdd = rdd.groupByKey()
reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)

-> collect(), count(),saveAsTextFile()
