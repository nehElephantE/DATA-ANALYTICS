####################
BIG DATA

Big data refers to extremely large and complex datasets that traditional data processing software can't effectively manage. It's characterized by the 
"Three Vs": Volume, Velocity, and Variety, with some definitions adding Variability and Veracity. Here's a detailed breakdown of these aspects:

### 1. Volume
- **Definition**: Refers to the sheer amount of data generated and collected. This can range from terabytes to petabytes and beyond.
- **Sources**: Data can come from various sources, including social media, sensors, transactional data, and more.
- **Challenges**: Storing and processing such large volumes requires scalable storage solutions and robust computational power.

### 2. Velocity
- **Definition**: This aspect addresses the speed at which data is generated and processed. Data can be created in real-time or near-real-time.
- **Examples**: Streaming data from social media, financial transactions, or IoT devices.
- **Challenges**: The need for real-time processing and analysis can overwhelm traditional data management systems.

### 3. Variety
- **Definition**: Refers to the different types of data (structured, semi-structured, unstructured) that can be collected.
- **Types**:
  - **Structured Data**: Organized data, typically in databases (e.g., SQL databases).
  - **Unstructured Data**: Data that doesn't have a predefined structure (e.g., text, images, videos).
  - **Semi-Structured Data**: Hybrid data formats that may contain both structured and unstructured elements (e.g., JSON, XML).
- **Challenges**: Integrating and analyzing diverse data types can complicate data management.

### 4. Variability
- **Definition**: Refers to the inconsistency of the data, which can change rapidly. This can impact data quality and analysis.
- **Example**: Seasonal trends in consumer behavior can create fluctuations in data patterns.
- **Challenges**: Maintaining data accuracy and relevance in the face of variability.

### 5. Veracity
- **Definition**: Addresses the quality and reliability of the data.
- **Issues**: Data can be noisy, incomplete, or biased, which can affect the outcomes of data analysis.
- **Challenges**: Ensuring data quality through validation and cleansing processes.




######################
HADOOP vs PYSPARK

1. Core Technology
Hadoop:
Hadoop is an open-source framework used for distributed storage and processing of large datasets. It primarily consists of 
two components:
HDFS (Hadoop Distributed File System): A distributed file system to store large datasets.
MapReduce: A programming model and engine for processing and generating large datasets in parallel.
PySpark:
PySpark is the Python API for Apache Spark, which is a fast, in-memory data processing engine. Spark is designed to be more 
efficient than Hadoop’s MapReduce by using memory for faster computation, which is often better suited for real-time analytics.

2. Programming Model
Hadoop:
MapReduce is the primary programming model for processing data in Hadoop. It involves splitting tasks into smaller sub-
tasks (mappers) and then combining the results (reducers). It's batch processing-based, which can be slower.
PySpark:
Spark uses a more flexible programming model, supporting APIs like RDDs (Resilient Distributed Datasets) and DataFrames for 
distributed data processing. PySpark provides a Python interface to interact with Spark’s capabilities, offering higher-level 
functions and faster processing than MapReduce.

3. Performance
Hadoop:
Hadoop’s MapReduce is disk-based, which means it stores intermediate data on disk. This can result in slower performance 
due to high I/O operations.
PySpark:
Spark is in-memory, meaning it processes data in RAM, significantly speeding up computations. This makes Spark much faster 
for iterative machine learning and real-time analytics tasks compared to Hadoop.

4. Ease of Use
Hadoop:
Writing MapReduce jobs can be complex and requires knowledge of Java (though other languages like Python can be used 
with additional frameworks).
PySpark:
PySpark is more user-friendly, especially for Python developers, because it provides a simpler and higher-level API to
process data using Python libraries.

5. Real-Time Processing
Hadoop:
Hadoop is primarily designed for batch processing. It’s not suited for real-time data processing.
PySpark:
Spark supports both batch processing and real-time stream processing with Spark Streaming. This makes it more versatile
for modern data processing needs, including real-time analytics.

6. Fault Tolerance
Hadoop:
Hadoop provides fault tolerance through data replication in HDFS. If a node fails, the data is replicated on other nodes 
to ensure no data loss.
PySpark:
Spark provides fault tolerance through RDDs. If a task fails, Spark recomputes the lost data using lineage information, 
ensuring no data loss.

7. Scalability
Hadoop:
Hadoop scales well by distributing data across a cluster and processing it in parallel. It is designed to handle petabytes
of data.
PySpark:
Spark also scales efficiently and is optimized for speed. It can process data in parallel across a cluster and handle 
very large datasets.

8. Use Cases
Hadoop:
Hadoop is typically used for batch processing and large-scale data storage. It’s well-suited for ETL (Extract, Transform, Load)
tasks, data warehousing, and data archiving.
PySpark:
PySpark is used for data analysis, machine learning, real-time processing, and data processing pipelines. It is popular 
for tasks like big data analytics, ETL, and processing streaming data.

9. Ecosystem
Hadoop:
Hadoop has a rich ecosystem of related tools, including:
Hive: SQL-like interface for querying data.
Pig: Scripting language for data processing.
HBase: NoSQL database for real-time access to large datasets.
PySpark:
PySpark integrates with the broader Apache Spark ecosystem, which includes tools like:
MLlib: Machine learning library.
GraphX: Graph processing framework.
Spark SQL: SQL-based querying interface.


#######################
PySPARK ARCHITECTURE

1. Driver Program: The Driver Program is the main entry point for PySpark applications. It runs the user’s code and coordinates the execution of tasks across the cluster.
The Driver Program communicates with the Cluster Manager to allocate resources, schedule tasks, and monitor their progress. It also manages the SparkContext, which is the
entry point for interacting with Spark from Python.

2. Cluster Manager: The Cluster Manager is responsible for managing the resources and scheduling tasks on a cluster of machines. It can be one of the supported cluster
managers, such as Apache Mesos, Hadoop YARN, or Spark’s standalone cluster manager. The Cluster Manager allocates resources to the application based on the requested 
configuration and manages the distribution of tasks to the available workers.

3. Executors: Executors are worker processes that run on the cluster nodes and perform the actual computations. They are responsible for executing the tasks assigned to
them by the Driver Program. Executors manage the data partitions and cache intermediate results in memory for efficient processing. They communicate with the Driver 
Program and the Cluster Manager to receive tasks, report task status, and exchange data.

4. Resilient Distributed Dataset (RDD): RDD is the fundamental data structure in PySpark. It represents an immutable distributed collection of objects across the cluster. 
RDDs are partitioned and distributed across multiple nodes in the cluster, allowing for parallel processing of data. RDDs are fault-tolerant, meaning they can recover 
from node failures by using their lineage information to reconstruct lost partitions.

Spark -> Scala(Advanced Java) -> provides API so its compatible with Python and Java 
Architecture -> Master node(Driver Code > Spark Content)
                Worker Nodes connected with master nodes using cluster manager

Spark Context is like the DB
RDD - Resililient (means flexible) Distributed Dataset  
RDD is schemaless

DAG - Directed Acyclic Graph is created upon creating RDD



#################
CREATION OF RDD

-> to parallelize(...) some collection of elements
-> by referencing a file located locally or externally


###################
TRANSFORMATIONS IN RDD

transformations are of 2 types - narrow and wide

-> map() : Applies a function func to each element in the RDD and returns a new RDD consisting of the results of the function.
rdd = sc.parallelize([1, 2, 3, 4])
squared_rdd = rdd.map(lambda x: x ** 2)

-> filter():Applies a function func to each element and returns an RDD that contains only the elements where func returns true.
rdd = sc.parallelize([1, 2, 3, 4, 5])
filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # Keep only even numbers

-> flatMap():Similar to map(), but each input item can be mapped to 0 or more output items (so func should return a sequence rather than a single item).
rdd = sc.parallelize(["hello world", "how are you"])
flat_mapped_rdd = rdd.flatMap(lambda x: x.split(" "))  # Split each sentence into words

-> disctinct():Returns a new RDD containing distinct elements from the original RDD.
rdd = sc.parallelize([1, 2, 2, 3, 3, 4])
distinct_rdd = rdd.distinct()

-> sample(withReplacement, fraction, seed=None):
rdd = sc.parallelize(range(100))
sampled_rdd = rdd.sample(False, 0.1)  # Sample 10% of the data without replacement

-> sortByKey(ascending=True):
rdd = sc.parallelize([(2, 'b'), (1, 'a'), (3, 'c')])
sorted_rdd = rdd.sortByKey()  # Sort by key in ascending order

-> union(other_rdd) and intersection(other_rdd):
rdd1 = sc.parallelize([1, 2, 3])
rdd2 = sc.parallelize([3, 4, 5])
union_rdd = rdd1.union(rdd2)
intersection_rdd = rdd1.intersection(rdd2)

-> groupByKey() and reduceByKey(func):
rdd = sc.parallelize([(1, 'a'), (2, 'b'), (1, 'c')])
grouped_rdd = rdd.groupByKey()
reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)

-> collect(), count(),saveAsTextFile()
